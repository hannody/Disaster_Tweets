{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as wtoken\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (Activation, Dropout, Dense, Embedding, Flatten, BatchNormalization, GlobalMaxPooling1D, Lambda, concatenate, LSTM, Concatenate)\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os, string, gc, re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import tldextract as tld\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In addition to Tensorflow, pandas and sklearn, this notebook requires the following NLP/Spacy packages.\n",
    "- conda install -c anaconda nltk\n",
    "- conda install -c conda-forge tldextract\n",
    "- conda install -c conda-forge wordcloud\n",
    "- conda install -c conda-forge rake_nltk\n",
    "- conda install -c conda-forge spacy\n",
    "- !python3 -m spacy download en_core_web_lg\n",
    "- !python3 -m spacy download en_core_web_md\n",
    "- !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick verification to see if the gpu drivers are ready to serve traning DL\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['keyword', 'location', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/moh/abunayla/Disaster_Tweets/Notebooks/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + 'train.csv', usecols = features)\n",
    "\n",
    "test = pd.read_csv(path + 'test.csv', usecols = features)\n",
    "\n",
    "samsub = pd.read_csv(path +'sample_submission.csv')\n",
    "\n",
    "Y =  pd.read_csv(path +'train.csv', usecols = ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsub.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keyword.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, start_with_char='#'):\n",
    "    \"\"\"\n",
    "    Take in string.\n",
    "    Extracts words start with certain chars.\n",
    "    Removes punctiouations\n",
    "    Returns extracted words that are longer than 3 chars.\n",
    "    \"\"\"\n",
    "    words=[]\n",
    "    words.append([word.translate(str.maketrans('', '', string.punctuation)) for word in text.split() if word.startswith(start_with_char) and len(word)>3 ])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keywords('this is a #Test, #ER if it works then It is a #Great solution.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'airplane%20accident'.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'airplane%20accident'.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wn.synsets('blaze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms =[]\n",
    "antonyms =[]\n",
    "for syn in wn.synsets('accident'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = wn.synset('fire.n.01')\n",
    "w2 = wn.synset('hazard.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\",pos =\"a\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "\n",
    "print(\"ablaze :\", lemmatizer.lemmatize(\"ablaze\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'better'\n",
    "print(lemmatizer.lemmatize(word)) \n",
    "print(lemmatizer.lemmatize(word, pos = 'a')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TweetTokenizer() method from nltk \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "  \n",
    "# Create a reference variable for Class TweetTokenizer \n",
    "tk = TweetTokenizer() \n",
    "  \n",
    "# Create a string input \n",
    "gfg = \"Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\"\n",
    "  \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "  \n",
    "print(geek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "words = [\"fire\", \"firing\", \"on_fire\", \"inflamable\", \"ablaze\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ablaze'\n",
    "print(word, \" : \", ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synset('fire.n.02').lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets('fire'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.keyword.fillna('#MISSING', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.keyword.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all words to lower, this helps when using NLTK functions since they are use lower case words, e.g. the stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "\n",
    "r.extract_keywords_from_text('Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding')\n",
    "\n",
    "\n",
    "r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'on the outside you are ablaze and alive but you are dead inside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "word_tokens = word_tokenize(text) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w)\n",
    "#print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(' '.join(filtered_sentence))\n",
    "\n",
    "\n",
    "r.get_ranked_phrases() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham\\'s Wholesale Market http://t.co/irWqCEZWEU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('running.n.01').lowest_common_hypernyms(wn.synset('accident.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking missing keyword against text if it has a keyword to fill the missing one in keyword column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keyword.equals('USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe I shoud use group by?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe I shoud use group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised learning for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as w_token\n",
    "text = w_token('Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham\\'s Wholesale Market http://t.co/irWqCEZWEU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag = nltk.pos_tag(text)\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        #if t.label() == 'NE':\n",
    "        #entity_names.append(' '.join([child[0] for child in t]))\n",
    "        #else:\n",
    "        for child in t:\n",
    "            entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = ['USA', 'London', 'Kuala Lumpur', 'Water', 'Car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = ['I will attend GDC my ass in china, wuhan city next year, then I will go to tanzania.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in msg:\n",
    "        sentences = nltk.sent_tokenize(line)\n",
    "        tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "        chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "        entities = []\n",
    "        for tree in chunked_sentences:\n",
    "            entities.extend(extract_entity_names(tree))\n",
    "\n",
    "        print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "text = 'I will attend GDC my ass in china, Mohanad wuhan city next year, then I will go to Tanzania, iraq.'\n",
    "for country in pycountry.countries:\n",
    "    if country.name.lower() in text:\n",
    "        print(country.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "text = 'I will attend GDC my ass in China, London wuhan city next year, then I will go to tanzania.'\n",
    "for country in pycountry.countries:\n",
    "    print(country.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China GPE\n",
      "London GPE\n",
      "wuhan GPE\n",
      "next year DATE\n",
      "tanzania GPE\n",
      "a Fool Phone PERSON\n",
      "Fish WORK_OF_ART\n",
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = nlp('I will attend GDC my ass in China, London wuhan next year, baghad then I will go to tanzania. Maybe I will buy a Fool Phone and a car, and will eat Fish and Apple')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China 28 33 GPE\n",
      "London 35 41 GPE\n",
      "next year 53 62 DATE\n",
      "tanzania 82 90 GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('I will attend GDC my ass in China, London wuhan city next year, then I will go to tanzania. Maybe I will buy a phone and a car')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n",
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # ['San', 'B', 'GPE']\n",
    "print(ent_francisco)  # ['Francisco', 'I', 'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- PRON PRP nsubj X True True \n",
      "\n",
      "will will VERB MD aux xxxx True True \n",
      "\n",
      "attend attend VERB VB ccomp xxxx True False \n",
      "\n",
      "GDC GDC PROPN NNP dobj XXX True False \n",
      "\n",
      "my -PRON- DET PRP$ poss xx True True \n",
      "\n",
      "ass ass NOUN NN dobj xxx True False \n",
      "\n",
      "in in ADP IN prep xx True True \n",
      "\n",
      "China China PROPN NNP pobj Xxxxx True False \n",
      "\n",
      ", , PUNCT , punct , False False \n",
      "\n",
      "London London PROPN NNP compound Xxxxx True False \n",
      "\n",
      "wuhan wuhan PROPN NNP compound xxxx True False \n",
      "\n",
      "city city NOUN NN npadvmod xxxx True False \n",
      "\n",
      "next next ADJ JJ amod xxxx True True \n",
      "\n",
      "year year NOUN NN npadvmod xxxx True False \n",
      "\n",
      ", , PUNCT , punct , False False \n",
      "\n",
      "then then ADV RB advmod xxxx True True \n",
      "\n",
      "I -PRON- PRON PRP nsubj X True True \n",
      "\n",
      "will will VERB MD aux xxxx True True \n",
      "\n",
      "go go VERB VB ROOT xx True True \n",
      "\n",
      "to to ADP IN prep xx True True \n",
      "\n",
      "tanzania tanzania PROPN NNP pobj xxxx True False \n",
      "\n",
      ". . PUNCT . punct . False False \n",
      "\n",
      "Maybe maybe ADV RB advmod Xxxxx True False \n",
      "\n",
      "I -PRON- PRON PRP nsubj X True True \n",
      "\n",
      "will will VERB MD aux xxxx True True \n",
      "\n",
      "buy buy VERB VB ROOT xxx True False \n",
      "\n",
      "a a DET DT det x True True \n",
      "\n",
      "phone phone NOUN NN dobj xxxx True False \n",
      "\n",
      "and and CCONJ CC cc xxx True True \n",
      "\n",
      "a a DET DT det x True True \n",
      "\n",
      "car car NOUN NN conj xxx True False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('I will attend GDC my ass in China, London wuhan city next year, then I will go to tanzania. Maybe I will buy a phone and a car')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@aria_ahrary @aria_ahrary ROOT @aria_ahrary\n",
      "control wild fires fires pobj of\n",
      "California California pobj in\n",
      "the Northern part part pobj in\n",
      "the state state pobj of\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.\")\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(\"schöne rote Äpfel auf dem Baum\")\n",
    "print([token.text for token in doc[2].lefts])  # ['schöne', 'rote']\n",
    "print([token.text for token in doc[2].rights])  # ['auf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 96.4 MB 1.1 MB/s eta 0:00:01     |███████████████████████▍        | 70.4 MB 874 kB/s eta 0:00:30\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (45.1.0.post20200127)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.9.6)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.3.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_md==2.2.5) (4.42.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/moh/anaconda3/envs/tf/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (2.1.0)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=aec0ac6e41d065b7eee4a637de2343996fbd3d1ff4d9af8ea3fbcf2fbeecf7fa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yz83r_3f/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
